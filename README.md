#  FURIA Chatbot

<img src="assets/Furia.png" alt="FURIA Logo" width="100"/>

Um chatbot inteligente especializado no time de CS2 da FURIA Esports. Responde perguntas sobre jogadores, partidas, estatÃ­sticas, loja oficial, e muito mais â€” utilizando uma LLM local com RAG em um modelo em formato GGUF.

### Acesse em: [FuriaChatbot](http://furiachat.kloresec.io/)

<img src="assets/mobilechat.png" alt="FURIA Logo" width="200"/> 
<img src="assets/chatdesktop.png" alt="FURIA Logo" width="400"/>
---

## ğŸš€ Tecnologias utilizadas

- âš™ï¸ **Backend**: [FastAPI](https://fastapi.tiangolo.com/)
- ğŸŒ **Frontend**: [Next.js](https://nextjs.org/)
- ğŸ§  **Modelo LLM**: [Nous Hermes 3 - LLaMA 3.1 8B (GGUF)](https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B-GGUF) com RAG
- ğŸ§¾ **Formato do modelo**: GGUF (usando `llama.cpp`)
- ğŸ” **Dados em tempo real**: Scraping da HLTV
  

---

## ğŸ“ Estrutura do Projeto

```
Furia-Chatbot/
â”œâ”€â”€ api/
     â””â”€â”€ models/Hermes-3-Llama...    # Modelo GGUF                       
â”œâ”€â”€ frontend/                    # Frontend Next.js
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ Como rodar o projeto localmente
### 0. Requsisitos Iniciais
  - RecomendÃ¡vel rodar em uma mÃ¡quina com pelo menos 8gb de Ram LIVRE<br>
  - Instale Python, pip e venv<br>
  - Instale o Node.js e npm<br>
  - Instale o Chrome + ChromeDriver em versÃµes compatÃ­veis 

### 1. Clone o repositÃ³rio

```bash
git clone https://github.com/PedroLarchert/furia-chatbot.git
cd furia-chatbot
```

### 2. Baixar modelo LLM 

```bash
cd Furiacs-Chatbot/api
python -m venv nome_do_ambiente (cria um ambioente virtual python)


nome_do_ambiente/bin/activate (Linux/macOS)
ou
nome_do_ambiente\Scripts\activate (Windows). (ativar o ambiente)

pip install -r requirements.txt
python download_model.py

```
### 3. Rodar o backend (FastAPI)

```bash

uvicorn main:app --reload
ou
fastapi dev main.py
```

### 4. Rodar o frontend (Next.js)

```bash
cd Furiacs-Chatbot/frontend
npm install
npm run dev
```
### ObservaÃ§Ãµes:
> - A velocidade das respostas do chat depende do poder computacional da mÃ¡quina, se a mÃ¡quina tiver nucleos Cuda, Ã© ainda melhor.<br>
> - Se tiver nucleos Cuda antes de iniciar de rodar o backend, VocÃª pode rodar LLAMA_CUBLAS=1 pip install llama-cpp-python --force-reinstall, para utilizÃ¡-los em vez de usar a CPU
> - O chat pode cometer erros e fugir do contexto em alguns casos, como utiliza um modelo pequeno e quantizado, ele pode fugir algumas vezes do prompt e alucinar, inventando informaÃ§Ãµes.<br>
> - O projeto atualmente utiliza o modelo Hermes 3 - LLaMA 3.1 8B em formato `.gguf`, rodando localmente com `llama-cpp-python`, mas tambÃ©m Ã© compatÃ­vel com execuÃ§Ã£o via binÃ¡rio manual (`llama-cli`) ou LM Studio.<br>
> - Em caso de problemas para rodar localmente, vocÃª pode acessar pelo link: [http://furiachat.kloresec.io/](http://furiachat.kloresec.io/)<br>

###  Outras formas de rodar o modelo LLM:

####  PadrÃ£o (integrado ao backend): `llama-cpp-python`

O modelo roda dentro do Python e responde diretamente via FastAPI.

```python
from llama_cpp import Llama
llm = Llama(model_path="api/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf")
```

####  Alternativa 1 â€“ CLI compilado (`llama.cpp`)

VocÃª pode clonar e compilar manualmente:

```bash
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
cmake -B build
cmake --build build
./build/bin/llama-cli -m path/para/seu/modelo.gguf
```

####  Alternativa 2 â€“ LM Studio

Ferramenta com interface grÃ¡fica que permite rodar o modelo e expor uma API local em:
```
http://localhost:1234/v1/chat/completions
```

Basta configurar o modelo `.gguf` e usar chamadas HTTP compatÃ­veis com OpenAI.

---

> O modelo deve estar salvo em `api/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf`
> o arquivo api/sendToLLM.py, possui o esqueleto para enviar requests http para a llm que estiver rodando em um servidor no modelo de chat/completions


---

## ğŸ“Œ Funcionalidades

- ğŸ§  Chatbot treinado com conhecimento sobre a FURIA e CS2
- ğŸ“… Agenda de partidas (via scraping)
- ğŸ“Š Resultado de jogadores
- ğŸ›’ InformaÃ§Ãµes sobre a loja oficial
- ğŸ‘Ãšltimos tweets no x (via scraping)

---

## Desafios enfrentados no projeto

Durante o desenvolvimento do projeto, alguns obstÃ¡culos tÃ©cnicos exigiram soluÃ§Ãµes alternativas. Entre os principais desafios, destacam-se:

#### Encontrar um modelo LLM reduzido e quantizado satisfatÃ³rio 
Depois de muitos testes com modelos de LLM, alguns modelos maiores inclusive, com 8B e 9B com quantizaÃ§Ã£o mais precisa, que nÃ£o obtiveram resultados tÃ£o bons quanto os desse pequeno modelo.<br>
De certo que hÃ¡ muitos modelos maiores, que trariam respostas melhores para esta tarefa,mas que exigiriam poder computacional muito grande para um processo local, fugindo do prÃ³posito deste projeto<br>
Inclusive uma opÃ§Ã£o para esse projeto seria utilizar as Apis de LLMs pagas como da OpenAI ou da Deepseek

#### Scraping e status ao vivo

O objetivo era mostrar o status "ao vivo" de partidas da FURIA,e integrar esses dados, ao conjunto de dados que faz parte do RAG para a LLM, incluindo placar parcial, mapas e informaÃ§Ãµes de picks/bans. Inicialmente, tentou-se fazer scraping do site da HLTV.org. No entanto, esse site implementa diversos bloqueios contra scraping nas pÃ¡gina com as informaÃ§Ãµes das partidas, como:

Cloudflare e verificaÃ§Ã£o JavaScript

Necessidade de simular um navegador real

MudanÃ§as frequentes na estrutura HTML

 SoluÃ§Ã£o utilizada: foi feito um script com Selenium e user-agent customizado para testes locais, inadequado para ambientes de produÃ§Ã£o, o mais adequado seria utilizar uma API paga oficial com suporte garantido a dados em tempo real.

---

## ğŸ‘¨â€ğŸ’» Autor

Desenvolvido por [Pedro VitÃ³rio Larchert de Oliveira](https://github.com/PedroLarchert)  
ğŸ“« Entre em contato para colaboraÃ§Ãµes!

---

## ğŸ“„ LicenÃ§a

Este projeto Ã© livre para uso educacional e experimental. Para uso comercial, entre em contato.
